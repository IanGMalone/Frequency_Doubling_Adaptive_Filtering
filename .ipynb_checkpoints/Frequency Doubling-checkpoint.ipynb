{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rubber-employment",
   "metadata": {},
   "source": [
    "# Frequency Doubling with Adaptive Filters\n",
    "\n",
    "Ian Malone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-granny",
   "metadata": {},
   "source": [
    "This problem can be called frequency doubling, because we want to learn the mapping that doubles the frequency of a single sine wave with an adaptive filter. Take the input at 1KHz sampled at 10KHz, and the desired response at 2KHz, and generate 2 seconds of data. \n",
    "\n",
    "\n",
    "Part 1) \n",
    "Use a linear FIR filter of size 10 trained with Wiener solution and LMS. Verify if the Wiener filter is able to solve this problem. Explain what happens in your own words. \n",
    "\n",
    "\n",
    "Part 2) \n",
    "Apply the KLMS to this problem (also using a vector of 10 input samples) and show that the solution is quite good. Experiment with different kernel sizes and also stepsizes to find the best performance. With the same parameters (i.e. no adaptation), change the input frequency between 500 Hz to 2KHz and show experimentally the generalization of the trained model. \n",
    "\n",
    "\n",
    "Part 3) \n",
    "Now repeat 2 with noise. Create a r.v. u obtained from a Gaussian mixture ùëù(ùë¢) = 0.9ùê∫(0,0.1) + 0.1ùê∫(4,0.1) and add the noise to the desired response. Quantify the effect of the noise in the model trained with MSE. Now implement the correntropy criterion (MCC) and compare the performance with the MSE. You have to properly determine the kernel size in MCC for optimal results. So\n",
    "show the effect of the kernel size in performance as a function the kernel size in the MCC cost. You can also experiment to test generalization as in 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-basement",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-finger",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "rental-diamond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 0 ns (started: 2021-03-30 11:26:14 -04:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy import linalg\n",
    "from statsmodels.tsa.stattools import acf,ccf\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-south",
   "metadata": {},
   "source": [
    "#### Make filter classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "international-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-03-30 11:26:08 -04:00)\n"
     ]
    }
   ],
   "source": [
    "class Wiener():\n",
    "    def __init__(self, order, start, length):\n",
    "        self.order = order\n",
    "        self.start = start\n",
    "        self.length = length\n",
    "        \n",
    "    def learn(self, input_signal, desired_signal):\n",
    "        '''Find optimal filter weights using Wiener filter given and input and output signal'''        \n",
    "        desired_signal = np.matrix(desired_signal).T\n",
    "        input_signal = np.matrix(input_signal).T\n",
    "        \n",
    "        A = np.matrix(np.zeros((self.length,self.order)))\n",
    "        \n",
    "        for i in range(self.length):\n",
    "            A[i,:] = input_signal[i+np.arange(self.order)].T # self.order or self.order-1 ???\n",
    "\n",
    "        R = A.T*A\n",
    "        P = A.T*desired_signal[self.start:self.length+self.start]\n",
    "        w_opt = np.linalg.inv(R)*P\n",
    "        \n",
    "        return w_opt\n",
    "        \n",
    "\n",
    "    \n",
    "class NLMS():\n",
    "    \n",
    "    def __init__(self, step_size, order):\n",
    "        self.step_size = step_size\n",
    "        self.order = order\n",
    "        \n",
    "    def learn(self, u, d):\n",
    "\n",
    "        # initialize\n",
    "        f = np.zeros(len(d))\n",
    "        e = np.zeros(len(d))\n",
    "        w = np.random.rand(self.order)\n",
    "        w_trk = np.zeros((len(d)-self.order, self.order))\n",
    "\n",
    "        # compute\n",
    "        for i in range(1, len(d)-self.order):\n",
    "            y = np.dot(w,u[i:i+self.order])\n",
    "            e[i] = d[i+self.order] - y\n",
    "            norm = ((np.linalg.norm(u[i:i+self.order])) ** 2)+0.1\n",
    "            w = w + (self.step_size * e[i] * u[i])/norm   #normalize the step by the power of the input\n",
    "            f[i] = y\n",
    "            w_trk[i] = w \n",
    "        return f, w_trk, e\n",
    "\n",
    "    \n",
    "    \n",
    "class GaussianKernel:\n",
    "    def kernel(self, a, b):\n",
    "        numer = (np.linalg.norm(a - b)) ** 2\n",
    "        denom = (2 * self.sigma ** 2)\n",
    "        return np.exp(-1 * (numer / denom))\n",
    "    \n",
    "    \n",
    "    \n",
    "class KLMS(GaussianKernel):\n",
    "    \n",
    "    def __init__(self, step_size, sigma, order): \n",
    "        self.step_size = step_size\n",
    "        self.sigma = sigma\n",
    "        self.order = order\n",
    "    \n",
    "    def learn(self, input_signal, desired_signal):\n",
    "        estimates = np.zeros(len(input_signal))\n",
    "        coefficients = np.zeros(len(input_signal))\n",
    "        errors = np.zeros(len(input_signal))\n",
    "        coefficients[0] = self.step_size * desired_signal[0]\n",
    "\n",
    "        for i in range(1, len(input_signal)-self.order):\n",
    "            for j in range(i-1):\n",
    "                \n",
    "                partial_sum = coefficients[j] * self.kernel(input_signal[i:i+(self.order)], input_signal[j:j+(self.order)])\n",
    "                estimates[i] += partial_sum                 \n",
    "\n",
    "            errors[i] = desired_signal[i+self.order] - estimates[i]\n",
    "            coefficients[i] = self.step_size * errors[i]\n",
    "        \n",
    "        return estimates, coefficients, errors\n",
    "    \n",
    "    \n",
    "    \n",
    "class QKLMS(GaussianKernel):\n",
    "    \n",
    "    def __init__(self, step_size, sigma, order, epsilon):\n",
    "        self.step_size = step_size\n",
    "        self.sigma = sigma\n",
    "        self.order = order\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def learn(self, input_signal, desired_signal):\n",
    "        estimates = np.zeros(len(input_signal))\n",
    "        errors = np.zeros(len(input_signal))\n",
    "        coefficients = np.zeros(len(input_signal))\n",
    "        coefficients[0] = self.step_size * desired_signal[0]\n",
    "            \n",
    "        center_vectors = np.empty([0, self.order]) ## HIGHTLIGHT\n",
    "        center_vectors = np.vstack((center_vectors, input_signal[:self.order]))\n",
    "        \n",
    "        distance = []\n",
    "        size = np.asarray(np.zeros(len(input_signal))) # these keeps track of the network size over iterations\n",
    "        size[0]=1\n",
    "        \n",
    "\n",
    "        for i in range(len(input_signal)-self.order):\n",
    "            for j in range(center_vectors.shape[0]):  ### HIGHTLIGHT AND LINE BELOW\n",
    "                partial_sum = coefficients[j] * self.kernel(input_signal[i:i+(self.order)], center_vectors[j])\n",
    "                estimates[i] += partial_sum                 \n",
    "                distance.append(np.linalg.norm(np.asarray(input_signal[i:i+(self.order)]) - np.asarray(center_vectors[j])))\n",
    "            \n",
    "            errors[i] = desired_signal[i] - estimates[i]  #should it really be desired_signal[i+self.order]\n",
    "            \n",
    "            if distance != []: # this probably shouldnt even be necesssary\n",
    "                dist = np.min(distance)\n",
    "                loc = np.argmin(distance)\n",
    "\n",
    "                if dist <= self.epsilon:\n",
    "                    coefficients[i] = coefficients[loc] + self.step_size * errors[i]\n",
    "                    size = np.append(size, size[-1])\n",
    "                    #size.append[size[-1]]\n",
    "                else:\n",
    "                    center_vectors = np.vstack((center_vectors, input_signal[i:i+(self.order)]))  ## HIGHTLIGHT np.vstack\n",
    "                    size = np.append(size, size[-1]+1)\n",
    "                    coefficients[i] = coefficients[i-1] + self.step_size * errors[i]\n",
    "                    #size.append[size[-1]+1]\n",
    "                \n",
    "            distance = []\n",
    "\n",
    "        return estimates, coefficients, errors, size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-marketplace",
   "metadata": {},
   "source": [
    "#### Define functions you may need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "indie-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-03-30 11:26:45 -04:00)\n"
     ]
    }
   ],
   "source": [
    "def ensemble_learning_curve(errors_matrix):\n",
    "    # ensemble of adaptive filters with same configuration settings\n",
    "    # get MSE vector for each filter\n",
    "    # average MSE across filters\n",
    "    errors = errors_matrix.mean(0)\n",
    "    curve = []\n",
    "    for i in range(len(errors)):\n",
    "        curve.append(np.mean(errors[:i]**2))\n",
    "    return curve\n",
    "\n",
    "def freq_response(e, d, fs):\n",
    "    return rfftfreq(len(e), 1/fs), abs(scipy.fft.rfft(e)/scipy.fft.rfft(d))\n",
    "\n",
    "def gaussian_kernel(a, b, sigma):\n",
    "    numer = (np.linalg.norm(a - b)) ** 2\n",
    "    denom = (2 * sigma ** 2)\n",
    "    return np.exp(-1 * (numer / denom))\n",
    "\n",
    "def ERLE(d, e):\n",
    "    return float(10*np.log10(sum(d**2)/sum(e**2)))\n",
    "\n",
    "def iqr(x):\n",
    "    q75, q25 = np.percentile(x, [75 ,25])\n",
    "    return q75 - q25\n",
    "\n",
    "def learning_curve(errors):\n",
    "    curve = []\n",
    "    for i in range(len(errors)):\n",
    "        curve.append(np.mean(errors[:i]**2))\n",
    "    return curve\n",
    "\n",
    "def klms_misadjustment(u, step_size, sigma):\n",
    "    # klms: p38 text..  \n",
    "    # data independent with shift-invariant kernels\n",
    "    # with Gaussian kernel, this evaluates to 0.5*step_size\n",
    "    kern = []\n",
    "    for i in u:\n",
    "        kern.append(gaussian_kernel(i, i, sigma))\n",
    "    return step_size*0.5*(1/(len(u)))*np.sum(kern)\n",
    "\n",
    "def klms_step(u, sigma):\n",
    "    # p38 text\n",
    "    kern = []\n",
    "    for i in u:\n",
    "        kern.append(gaussian_kernel(i, i, sigma))\n",
    "    return len(u)/np.sum(kern)\n",
    "\n",
    "def nlms_misadjustment(u, step_size):\n",
    "    # nlms: p30 text.. \n",
    "    # 0.5*step_size*tr[R] = 0.5*step_size*mean*square\n",
    "    return 0.5*step_size*np.mean(u**2)\n",
    "\n",
    "def nlms_step(u):\n",
    "    # p30 textbook\n",
    "    return 1/(np.mean(u**2))\n",
    "\n",
    "def silverman_bandwidth(x):\n",
    "    return 1.06*np.min([np.std(x),iqr(x)/1.34])*(len(x)**(-1/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-reconstruction",
   "metadata": {},
   "source": [
    "#### Create signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "essential-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2021-03-30 11:23:35 -04:00)\n"
     ]
    }
   ],
   "source": [
    "fs = 10000\n",
    "time = np.arange(0, 2, 1/fs)\n",
    "\n",
    "# generate input signal (2 second sine wave, 1 kHz frequency, 10 kHz sampling)\n",
    "f_in = 1000\n",
    "w_in = 2. * np.pi * f_in\n",
    "sine_1k = np.sin(w_in * time)\n",
    "\n",
    "# generate desired signal (2 second sine wave, 2 kHz frequency, 10 kHz sampling)\n",
    "f_des = 2000\n",
    "w_des = 2. * np.pi * f_des\n",
    "sine_2k = np.sin(w_des * time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-essay",
   "metadata": {},
   "source": [
    "Part 1) \n",
    "Use a linear FIR filter of size 10 trained with Wiener solution and LMS. Verify if the Wiener filter is able to solve this problem. Explain what happens in your own words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-permission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-front",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-shipping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-future",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-mexican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-shadow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-cyprus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "composed-composite",
   "metadata": {},
   "source": [
    "Part 2) \n",
    "Apply the KLMS to this problem (also using a vector of 10 input samples) and show that the solution is quite good. Experiment with different kernel sizes and also stepsizes to find the best performance. With the same parameters (i.e. no adaptation), change the input frequency between 500 Hz to 2KHz and show experimentally the generalization of the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-secondary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-haven",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-bearing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-clark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-tennis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "casual-stations",
   "metadata": {},
   "source": [
    "Part 3) \n",
    "Now repeat 2 with noise. Create a r.v. u obtained from a Gaussian mixture ùëù(ùë¢) = 0.9ùê∫(0,0.1) + 0.1ùê∫(4,0.1) and add the noise to the desired response. Quantify the effect of the noise in the model trained with MSE. Now implement the correntropy criterion (MCC) and compare the performance with the MSE. You have to properly determine the kernel size in MCC for optimal results. So\n",
    "show the effect of the kernel size in performance as a function the kernel size in the MCC cost. You can also experiment to test generalization as in 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-enterprise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-advantage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-prairie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-supplement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-emphasis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-penguin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-police",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
